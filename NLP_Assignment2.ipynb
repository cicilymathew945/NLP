{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8bac488c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import data\n",
    "import pandas as pd\n",
    "url = \"https://storage.googleapis.com/adsp-nlp-open/data/elsevier-oa-cc-by/abstracts.json\"\n",
    "data = pd.read_json(url, orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d859339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['phys_sci' 'healh_sci' 'engi_tech' 'life_sci' 'soc_sci']\n"
     ]
    }
   ],
   "source": [
    "# View the target data columns\n",
    "data.head()\n",
    "subject=data['subject'].unique()\n",
    "print(subject)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5a4c8d",
   "metadata": {},
   "source": [
    "The data comes with \"title\" and \"abstract\" fields. You may use either (or both) for your modeling.\n",
    "Using both gave better results hence using it to compare the models using techniques of BOW and Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ba23dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import labelencoder and combine 2 columns\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoder=LabelEncoder()\n",
    "#fit the encoder\n",
    "data['subject_encoded']=encoder.fit_transform(data['subject'])\n",
    "data['text']=data['abstract']+' ' +data['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c03221d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean\n",
    "import re\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)  # Remove special characters\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Remove extra spaces\n",
    "    text=text.lower()  # Convert to lowercase\n",
    "    return text.strip() \n",
    "\n",
    "data['cleaned_text'] = data['text'].apply(clean_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd98a1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#vectorization using bag of words\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(max_features=1000, stop_words='english')  # Limit to 1000 features for simplicity\n",
    "X = vectorizer.fit_transform(data['cleaned_text'])\n",
    "#target variable\n",
    "y = data['subject_encoded']\n",
    "#get feature names\n",
    "words=vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44e63411",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0868fb95",
   "metadata": {},
   "source": [
    "#Model 1: Ensemble of 1000 logistic regression model using BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a272a720",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6539157405014215\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.70      0.71      1809\n",
      "           1       0.75      0.72      0.73      1525\n",
      "           2       0.60      0.47      0.53      1191\n",
      "           3       0.60      0.68      0.64      2578\n",
      "           4       0.60      0.60      0.60       635\n",
      "\n",
      "    accuracy                           0.65      7738\n",
      "   macro avg       0.65      0.63      0.64      7738\n",
      "weighted avg       0.66      0.65      0.65      7738\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Step 1: Vectorize the text data using CountVectorizer\n",
    "vectorizer = CountVectorizer(max_features=1000, stop_words='english')  # Limit to 1000 features for simplicity\n",
    "X = vectorizer.fit_transform(data['cleaned_text'])\n",
    "\n",
    "# Step 2: Define the target variable\n",
    "y = data['subject_encoded']\n",
    "\n",
    "# Step 3: Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 4: Initialize the Logistic Regression model\n",
    "classifier1 = LogisticRegression(max_iter=1000)\n",
    "\n",
    "# Step 5: Create an ensemble of Logistic Regression models using BaggingClassifier\n",
    "ensemble = BaggingClassifier(estimator=classifier1, n_estimators=1000, random_state=42, n_jobs=-1)\n",
    "\n",
    "# Step 6: Train the ensemble model\n",
    "ensemble.fit(X_train, y_train)\n",
    "\n",
    "# Step 7: Predict on the test set\n",
    "y_pred = ensemble.predict(X_test)\n",
    "\n",
    "# Step 8: Evaluate the performance of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "\n",
    "# Print results\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(\"Classification Report:\")\n",
    "print(class_report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1f4bf8",
   "metadata": {},
   "source": [
    "#Model 2: Using Logistic regression with Lemmatization for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "741b0d6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\cicily.mathew\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\cicily.mathew\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6561126906177307\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.71      0.71      1809\n",
      "           1       0.74      0.72      0.73      1525\n",
      "           2       0.61      0.46      0.53      1191\n",
      "           3       0.60      0.68      0.64      2578\n",
      "           4       0.60      0.62      0.61       635\n",
      "\n",
      "    accuracy                           0.66      7738\n",
      "   macro avg       0.65      0.64      0.64      7738\n",
      "weighted avg       0.66      0.66      0.65      7738\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Initialize lemmatizer and stopwords\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Function to clean and lemmatize text\n",
    "def preprocess_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove punctuation and special characters\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    \n",
    "    # Tokenize and remove stopwords\n",
    "    tokens = [word for word in text.split() if word not in stop_words]\n",
    "    \n",
    "    # Lemmatize each word\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    \n",
    "    # Join the tokens back into a string\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# Assuming 'data' is a DataFrame with 'cleaned_abstract' and 'subject_encoded' columns\n",
    "data['cleaned_text_lemmatized'] = data['cleaned_text'].apply(preprocess_text)\n",
    "\n",
    "# Vectorize the cleaned abstract using CountVectorizer or TfidfVectorizer\n",
    "vectorizer = CountVectorizer(max_features=1000)  # Limit to 1000 features for simplicity\n",
    "XX = vectorizer.fit_transform(data['cleaned_text_lemmatized'])\n",
    "\n",
    "# Target variable\n",
    "y = data['subject_encoded']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "XX_train, XX_test, y_train, y_test = train_test_split(XX, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the Logistic Regression classifier\n",
    "classifier2 = LogisticRegression(max_iter=500)  # Increase max_iter to ensure convergence\n",
    "\n",
    "# Train the model\n",
    "classifier2.fit(XX_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = classifier2.predict(XX_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "\n",
    "# Print results\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(\"Classification Report:\")\n",
    "print(class_report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7ec994",
   "metadata": {},
   "source": [
    "#Model3: Random forest Classifier with BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c8cf3d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6492633755492375\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.74      0.71      1809\n",
      "           1       0.72      0.80      0.76      1525\n",
      "           2       0.69      0.30      0.42      1191\n",
      "           3       0.59      0.71      0.64      2578\n",
      "           4       0.62      0.45      0.52       635\n",
      "\n",
      "    accuracy                           0.65      7738\n",
      "   macro avg       0.66      0.60      0.61      7738\n",
      "weighted avg       0.65      0.65      0.64      7738\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Use Random forest to predict the text\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# Initialize the model\n",
    "classifier3 = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "# Train the classifier on the training data\n",
    "classifier3.fit(X_train, y_train)\n",
    "# Predict on the test set\n",
    "y_pred_RF3 = classifier3.predict(X_test)\n",
    "# Evaluate performance\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_RF3))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_RF3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113afd4d",
   "metadata": {},
   "source": [
    "What kind of models did you try and why?\n",
    "\n",
    "Tried different variations of Logistic regression with each column seperately and combined along with SVM and random forest classifier with techniques BOW and lemmatization. Final 3 models are #Model 1: Ensemble of 1000 logistic regression model using BOW, #Model 2: Using Logistic regression with Lemmatization for prediction, #Model3: Random forest Classifier with BOW."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa36cbda",
   "metadata": {},
   "source": [
    "How did you evaluate the model and which metric do you think is most important.\n",
    "Focus on evaluating the model based on accuracy rather than performance, as the university prioritizes relevance for search results. The model can run overnight to classify the documents, so performance is less of a concern."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adef7eea",
   "metadata": {},
   "source": [
    "How did you investigate misclassifications? Ans: Used classification report to investigate misclassifications, as it provides detailed metrics for evaluating the performance of all the classification model. It includes Precision, Recall, F1-Score, and Support for each class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a25a07",
   "metadata": {},
   "source": [
    "Were the misclassifications understandable (genuinely difficult examples) or were they blatant errors? Provide a few examples."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
